{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyM/p+ah6mT9/y2k9CZpYWwN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/biniwollo/DSA-5900/blob/main/SetUpTFFforFiveCompanies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-by-Step Guide to Setting Up Federated Learning with TensorFlow Federated:\n",
        "1. Install TensorFlow Federated\n",
        "\n",
        "You first need to install the tensorflow-federated library.\n",
        "\n",
        "In Google Colab, or your local Python environment, run:"
      ],
      "metadata": {
        "id": "pVKG_jkpKNHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install TensorFlow Federated"
      ],
      "metadata": {
        "id": "RrsKY-CShBgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import TensorFlow\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFc454D2bT1J",
        "outputId": "5fe719b1-65bf-414e-c2f4-68e5755b0049"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install tensorflow federated\n",
        "!pip install tensorflow-federated\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HSABqPDUgzL7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Import TensorFlow Federated"
      ],
      "metadata": {
        "id": "DvIJCacmh1E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_federated as tff\n",
        "\n",
        "# Verify TensorFlow Federated version\n",
        "print(f\"TensorFlow Federated Version: {tff.__version__}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWBmERLTUexu",
        "outputId": "652a2ffa-238f-4b35-fa77-61065ca54eb5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Federated Version: 0.87.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve Stock Data from Yahoo Finance"
      ],
      "metadata": {
        "id": "VrrFwLHikRLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance"
      ],
      "metadata": {
        "id": "ySfLiO05kd88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "6e3c1e44-d0a9-47eb-d7f7-a0a0f36563d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.40)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.25.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.31.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.4)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.2.2)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2023.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.4)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.6)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the companies and their ticker symbols\n",
        "companies = {\n",
        "    'John Deere': 'DE',\n",
        "    'Archer-Daniels-Midland': 'ADM',\n",
        "    'Bunge Ltd': 'BG',\n",
        "    'The Mosaic Company': 'MOS',\n",
        "    'Corteva': 'CTVA'\n",
        "}\n",
        "\n",
        "# Set up directory in the default Colab environment\n",
        "base_dir = '/content/FinancialData'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Loop through each company and download the stock data\n",
        "for company, ticker in companies.items():\n",
        "    print(f\"Downloading data for {company} ({ticker})...\")\n",
        "    stock_data = yf.download(ticker, start='2019-09-16', end='2024-09-13')\n",
        "    file_path = os.path.join(base_dir, f\"{ticker}_stock_data.csv\")\n",
        "    stock_data.to_csv(file_path)\n",
        "    print(f\"Data for {company} ({ticker}) saved successfully at {file_path}\")\n",
        "\n",
        "# Combine all data into a single CSV (optional)\n",
        "combined_file_path = os.path.join(base_dir, \"combined_stock_data.csv\")\n",
        "combined_stock_data = pd.concat([pd.read_csv(os.path.join(base_dir, f\"{ticker}_stock_data.csv\")) for ticker in companies.values()])\n",
        "combined_stock_data.to_csv(combined_file_path, index=False)\n",
        "print(f\"Combined stock data saved at: {combined_file_path}\")\n"
      ],
      "metadata": {
        "id": "Z3FrM1QekVwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "246ad908-b38f-4df3-a34c-f56b6dcddcf9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data for John Deere (DE)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for John Deere (DE) saved successfully at /content/FinancialData/DE_stock_data.csv\n",
            "Downloading data for Archer-Daniels-Midland (ADM)...\n",
            "Data for Archer-Daniels-Midland (ADM) saved successfully at /content/FinancialData/ADM_stock_data.csv\n",
            "Downloading data for Bunge Ltd (BG)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for Bunge Ltd (BG) saved successfully at /content/FinancialData/BG_stock_data.csv\n",
            "Downloading data for The Mosaic Company (MOS)...\n",
            "Data for The Mosaic Company (MOS) saved successfully at /content/FinancialData/MOS_stock_data.csv\n",
            "Downloading data for Corteva (CTVA)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for Corteva (CTVA) saved successfully at /content/FinancialData/CTVA_stock_data.csv\n",
            "Combined stock data saved at: /content/FinancialData/combined_stock_data.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that both TensorFlow (2.14.0) and TensorFlow Federated (0.87.0) are successfully installed and ready to use, we're all set to start working on your project. Let's begin our financial analysis using TensorFlow Federated Learning with the stock data, here's a simple structure to get started:\n",
        "Steps for Using TensorFlow Federated Learning:\n",
        "\n",
        "1. Prepare the Dataset: Organize our stock data for each company into client datasets for federated learning.\n",
        "2. Define the Model: Build a machine learning model (e.g., for stock price prediction or volatility analysis).\n",
        "3. Federated Training: Set up the federated learning process where each company acts as a client in the federated environment.\n",
        "4. Evaluate the Model: Assess the performance of the federated model across all clients.\n",
        "\n",
        "Here's an outline of how you can structure the code:"
      ],
      "metadata": {
        "id": "lYvC2k8Git6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the dataset:\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to the saved stock data in Colab\n",
        "base_dir = '/content/FinancialData'\n",
        "\n",
        "# Load each company's stock data from the saved CSV files\n",
        "def load_company_data(ticker):\n",
        "    file_path = os.path.join(base_dir, f\"{ticker}_stock_data.csv\")\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Define a function to load stock data for all companies and convert it to tf.data.Dataset\n",
        "def create_client_data():\n",
        "    companies = ['DE', 'ADM', 'BG', 'MOS', 'CTVA']  # List of ticker symbols\n",
        "    client_data = []\n",
        "\n",
        "    for ticker in companies:\n",
        "        data = load_company_data(ticker)\n",
        "        # Use relevant features for the model (e.g., Close price, Volume)\n",
        "        features = data[['Close', 'Volume']].fillna(0).values.astype('float32') # Changed to float32\n",
        "        labels = data[['Close']].fillna(0).values.astype('float32')  # Predicting Close price, changed to float32\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "        dataset = dataset.batch(32)  # Batch the dataset\n",
        "        client_data.append(dataset)\n",
        "    return client_data\n",
        "\n",
        "# Create a simple model for stock price prediction (e.g., linear regression)\n",
        "def create_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu', input_shape=(2,)),  # 2 features: Close price, Volume\n",
        "        tf.keras.layers.Dense(1)  # Output layer for predicting next price\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Federated learning process setup\n",
        "def federated_training():\n",
        "    client_data = create_client_data()\n",
        "\n",
        "    # Define input specification\n",
        "    input_spec = {\n",
        "        'x': tf.TensorSpec(shape=[None, 2], dtype=tf.float32),\n",
        "        'y': tf.TensorSpec(shape=[None, 1], dtype=tf.float32)\n",
        "    }\n",
        "\n",
        "    # Create a TFF model\n",
        "    def model_fn():\n",
        "        keras_model = create_model()\n",
        "        return tff.learning.models.from_keras_model(\n",
        "            keras_model=keras_model,\n",
        "            input_spec=input_spec,\n",
        "            loss=tf.keras.losses.MeanSquaredError(),\n",
        "            metrics=[tf.keras.metrics.MeanSquaredError()]\n",
        "        )\n",
        "\n",
        "    # Use TFF's internal sgdm optimizer instead of Keras's optimizer\n",
        "    client_optimizer_fn = tff.learning.optimizers.build_sgdm(learning_rate=0.01)\n",
        "    server_optimizer_fn = tff.learning.optimizers.build_sgdm(learning_rate=1.0)\n",
        "\n",
        "    # Build federated averaging process using TFF's internal optimizers\n",
        "    iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "        model_fn=model_fn,\n",
        "        client_optimizer_fn=client_optimizer_fn,  # Pass the optimizer directly\n",
        "        server_optimizer_fn=server_optimizer_fn   # Pass the optimizer directly\n",
        "    )\n",
        "\n",
        "    # Initialize the model state\n",
        "    state = iterative_process.initialize()\n",
        "\n",
        "    # Simulate training across clients (5 companies)\n",
        "    for round_num in range(1, 9):\n",
        "        state, metrics = iterative_process.next(state, client_data)\n",
        "        print(f'Round {round_num}, Metrics: {metrics}')\n",
        "\n",
        "# Start federated training\n",
        "federated_training()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKxgPqGO-HUe",
        "outputId": "e23e14ff-7a9d-469f-b9a2-903e4386374d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 1, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 2, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 3, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 4, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 5, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 6, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 7, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 8, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enhance the Model Architecture"
      ],
      "metadata": {
        "id": "hckkDBx0YKyV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kOmx0SkP6yky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ZqcHIFP6yLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z8YJfXUG6x-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to the saved stock data in Colab\n",
        "base_dir = '/content/FinancialData'\n",
        "\n",
        "# Load each company's stock data from the saved CSV files\n",
        "def load_company_data(ticker):\n",
        "    file_path = os.path.join(base_dir, f\"{ticker}_stock_data.csv\")\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Define a function to load stock data for all companies and convert it to tf.data.Dataset\n",
        "def create_client_data():\n",
        "    companies = ['DE', 'ADM', 'BG', 'MOS', 'CTVA']  # List of ticker symbols\n",
        "    client_data = []\n",
        "\n",
        "    for ticker in companies:\n",
        "        data = load_company_data(ticker)\n",
        "        # Debug: Check if data is loaded correctly\n",
        "        #print(f\"Data for {ticker} loaded. Sample:\\n\", data.head())\n",
        "\n",
        "        # Use relevant features for the model (e.g., Close price, Volume)\n",
        "        features = data[['Close', 'Volume']].fillna(0).values.astype('float32')  # Changed to float32\n",
        "        labels = data[['Close']].fillna(0).values.astype('float32')  # Predicting Close price, changed to float32\n",
        "\n",
        "        # Debug: Check features and labels\n",
        "        #print(f\"Features shape for {ticker}: {features.shape}\")\n",
        "        #print(f\"Labels shape for {ticker}: {labels.shape}\")\n",
        "\n",
        "        # Create dataset and check batches\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "        dataset = dataset.batch(32)  # Batch the dataset\n",
        "        client_data.append(dataset)\n",
        "\n",
        "        # Debug: Check batched dataset\n",
        "        #print(f\"Batched dataset for {ticker}:\")\n",
        "        for batch in dataset:\n",
        "            #print(batch)\n",
        "            break  # Print only the first batch for debugging\n",
        "\n",
        "    return client_data\n",
        "\n",
        "# Create a simple model for stock price prediction (e.g., linear regression)\n",
        "def create_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu', input_shape=(2,)),  # 2 features: Close price, Volume\n",
        "        tf.keras.layers.Dense(1)  # Output layer for predicting next price\n",
        "    ])\n",
        "\n",
        "    # Debug: Check model summary\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "# Define a TFF model\n",
        "def model_fn():\n",
        "    try:\n",
        "        # Create the Keras model using the function defined earlier\n",
        "        keras_model = create_model()\n",
        "\n",
        "        # Ensure input_spec matches the dataset structure (features, labels)\n",
        "        input_spec = (\n",
        "            tf.TensorSpec(shape=[None, 2], dtype=tf.float32),  # Features (2: Close price, Volume)\n",
        "            tf.TensorSpec(shape=[None, 1], dtype=tf.float32)   # Labels (1: Close price prediction)\n",
        "        )\n",
        "\n",
        "        # Create a TFF model from the Keras model\n",
        "        tff_model = tff.learning.models.from_keras_model(\n",
        "            keras_model=keras_model,\n",
        "            input_spec=input_spec,\n",
        "            loss=tf.keras.losses.MeanSquaredError(),  # Loss function (regression task)\n",
        "            metrics=[tf.keras.metrics.MeanSquaredError()]  # Metrics to track\n",
        "        )\n",
        "\n",
        "        print(\"TFF model created successfully.\")\n",
        "\n",
        "        return tff_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while creating TFF model: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Federated learning process setup\n",
        "def federated_training():\n",
        "    client_data = create_client_data()\n",
        "\n",
        "    # Use TFF's internal sgdm optimizer instead of Keras's optimizer\n",
        "    client_optimizer_fn = tff.learning.optimizers.build_sgdm(learning_rate=0.01)\n",
        "    server_optimizer_fn = tff.learning.optimizers.build_sgdm(learning_rate=1.0)\n",
        "\n",
        "    # Build federated averaging process using TFF's internal optimizers\n",
        "    iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "        model_fn=model_fn,\n",
        "        client_optimizer_fn=client_optimizer_fn,  # Pass the optimizer directly\n",
        "        server_optimizer_fn=server_optimizer_fn   # Pass the optimizer directly\n",
        "    )\n",
        "\n",
        "    # Initialize the model state\n",
        "    state = iterative_process.initialize()\n",
        "\n",
        "    # Simulate training across clients (5 companies)\n",
        "    for round_num in range(1, 11):\n",
        "        state, metrics = iterative_process.next(state, client_data)\n",
        "        print(f'Round {round_num}, Metrics: {metrics}')\n",
        "\n",
        "# Start federated training\n",
        "federated_training()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4qJ4rBo62GM",
        "outputId": "8d90342f-2606-4a6b-b0f0-9c06d5ceac32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                30        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41 (164.00 Byte)\n",
            "Trainable params: 41 (164.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "TFF model created successfully.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                30        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41 (164.00 Byte)\n",
            "Trainable params: 41 (164.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "TFF model created successfully.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                30        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41 (164.00 Byte)\n",
            "Trainable params: 41 (164.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "TFF model created successfully.\n",
            "Round 1, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 78207885000.0), ('loss', 78207885000.0), ('num_examples', 6285), ('num_batches', 200)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 2, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 96265000.0), ('loss', 96265000.0), ('num_examples', 6285), ('num_batches', 200)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 3, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 19102316.0), ('loss', 19102316.0), ('num_examples', 6285), ('num_batches', 200)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 4, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 3788853.0), ('loss', 3788853.0), ('num_examples', 6285), ('num_batches', 200)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 5, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 753456.5), ('loss', 753456.5), ('num_examples', 6285), ('num_batches', 200)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 6, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 153422.25), ('loss', 153422.25), ('num_examples', 6285), ('num_batches', 200)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 7, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 35538.15), ('loss', 35538.15), ('num_examples', 6285), ('num_batches', 200)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 8, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 12705.355), ('loss', 12705.355), ('num_examples', 6285), ('num_batches', 200)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 9, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 8430.274), ('loss', 8430.274), ('num_examples', 6285), ('num_batches', 200)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 10, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 7697.195), ('loss', 7697.195), ('num_examples', 6285), ('num_batches', 200)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xRtQMfxH84RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9aOmQhqo83l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to the saved stock data in Colab\n",
        "base_dir = '/content/FinancialData'\n",
        "\n",
        "# Load each company's stock data from the saved CSV files\n",
        "def load_company_data(ticker):\n",
        "    file_path = os.path.join(base_dir, f\"{ticker}_stock_data.csv\")\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Define a function to load stock data for all companies and convert it to tf.data.Dataset\n",
        "def create_client_data():\n",
        "    companies = ['DE', 'ADM', 'BG', 'MOS', 'CTVA']  # List of ticker symbols\n",
        "    client_data = []\n",
        "\n",
        "    for ticker in companies:\n",
        "        data = load_company_data(ticker)\n",
        "        # Debug: Check if data is loaded correctly\n",
        "        #print(f\"Data for {ticker} loaded. Sample:\\n\", data.head())\n",
        "\n",
        "        # Use relevant features for the model (e.g., Close price, Volume)\n",
        "        features = data[['Close', 'Volume']].fillna(0).values.astype('float32')  # Changed to float32\n",
        "        labels = data[['Close']].fillna(0).values.astype('float32')  # Predicting Close price, changed to float32\n",
        "\n",
        "        # Debug: Check features and labels\n",
        "        #print(f\"Features shape for {ticker}: {features.shape}\")\n",
        "        #print(f\"Labels shape for {ticker}: {labels.shape}\")\n",
        "\n",
        "        # Create dataset and check batches\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "        dataset = dataset.batch(32)  # Batch the dataset\n",
        "        client_data.append(dataset)\n",
        "\n",
        "        # Debug: Check batched dataset\n",
        "        #print(f\"Batched dataset for {ticker}:\")\n",
        "        for batch in dataset:\n",
        "            #print(batch)\n",
        "            break  # Print only the first batch for debugging\n",
        "\n",
        "    return client_data\n",
        "\n",
        "# Create a simple model for stock price prediction (e.g., linear regression)\n",
        "def create_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu', input_shape=(2,)),  # 2 features: Close price, Volume\n",
        "        tf.keras.layers.Dense(1)  # Output layer for predicting next price\n",
        "    ])\n",
        "\n",
        "    # Debug: Check model summary\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "# Define a TFF model\n",
        "def model_fn():\n",
        "    try:\n",
        "        # Create the Keras model using the function defined earlier\n",
        "        keras_model = create_model()\n",
        "\n",
        "        # Ensure input_spec matches the dataset structure (features, labels)\n",
        "        input_spec = (\n",
        "            tf.TensorSpec(shape=[None, 2], dtype=tf.float32),  # Features (2: Close price, Volume)\n",
        "            tf.TensorSpec(shape=[None, 1], dtype=tf.float32)   # Labels (1: Close price prediction)\n",
        "        )\n",
        "\n",
        "        # Create a TFF model from the Keras model\n",
        "        tff_model = tff.learning.models.from_keras_model(\n",
        "            keras_model=keras_model,\n",
        "            input_spec=input_spec,\n",
        "            loss=tf.keras.losses.MeanSquaredError(),  # Loss function (regression task)\n",
        "            metrics=[tf.keras.metrics.MeanSquaredError()]  # Metrics to track\n",
        "        )\n",
        "\n",
        "        #print(\"TFF model created successfully.\")\n",
        "\n",
        "        return tff_model\n",
        "\n",
        "    except Exception as e:\n",
        "        #print(f\"Error while creating TFF model: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Federated learning process setup\n",
        "def federated_training():\n",
        "    client_data = create_client_data()\n",
        "\n",
        "    # Use TFF's internal sgdm optimizer instead of Keras's optimizer\n",
        "    client_optimizer_fn = tff.learning.optimizers.build_sgdm(learning_rate=0.01)\n",
        "    server_optimizer_fn = tff.learning.optimizers.build_sgdm(learning_rate=1.0)\n",
        "\n",
        "    # Build federated averaging process using TFF's internal optimizers\n",
        "    iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "        model_fn=model_fn,\n",
        "        client_optimizer_fn=client_optimizer_fn,  # Pass the optimizer directly\n",
        "        server_optimizer_fn=server_optimizer_fn   # Pass the optimizer directly\n",
        "    )\n",
        "\n",
        "    # Initialize the model state\n",
        "    state = iterative_process.initialize()\n",
        "\n",
        "    # Simulate training across clients (5 companies)\n",
        "    for round_num in range(1, 11):\n",
        "        state, metrics = iterative_process.next(state, client_data)\n",
        "        print(f'Round {round_num}, Metrics: {metrics}')\n",
        "\n",
        "# Start federated training\n",
        "federated_training()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBvQ_M177qmM",
        "outputId": "4c47fe14-7d10-4455-be6d-87efb725c532"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                30        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41 (164.00 Byte)\n",
            "Trainable params: 41 (164.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                30        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41 (164.00 Byte)\n",
            "Trainable params: 41 (164.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                30        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41 (164.00 Byte)\n",
            "Trainable params: 41 (164.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Round 1, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 2, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 3, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 4, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 5, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 6, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 7, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 8, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 9, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 10, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JdOfLVgr8gBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not data.empty:\n",
        "    print(f\"Data loaded for {ticker}\")\n",
        "else:\n",
        "    print(f\"No data for {ticker}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "ZdidwkEn69KW",
        "outputId": "aefaf894-9851-44d7-dd1c-030ef1c5e295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8a331488c057>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Data loaded for {ticker}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No data for {ticker}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Path to the saved stock data in Colab\n",
        "base_dir = '/content/FinancialData'\n",
        "\n",
        "# Load each company's stock data from the saved CSV files\n",
        "def load_company_data(ticker):\n",
        "    file_path = os.path.join(base_dir, f\"{ticker}_stock_data.csv\")\n",
        "\n",
        "    # Check if file exists\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"File {file_path} does not exist.\")\n",
        "        return pd.DataFrame()  # Return empty dataframe if file does not exist\n",
        "\n",
        "    try:\n",
        "        data = pd.read_csv(file_path)\n",
        "\n",
        "        # Check if the data is empty or not\n",
        "        if data.empty:\n",
        "            print(f\"No data found for {ticker}\")\n",
        "        else:\n",
        "            print(f\"Data loaded for {ticker}: {len(data)} rows.\")\n",
        "\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data for {ticker}: {e}\")\n",
        "        return pd.DataFrame()  # Return empty dataframe in case of error\n",
        "\n",
        "# Define a function to load stock data for all companies and convert it to tf.data.Dataset\n",
        "def create_client_data():\n",
        "    companies = ['DE', 'ADM', 'BG', 'MOS', 'CTVA']  # List of ticker symbols\n",
        "    client_data = []\n",
        "\n",
        "    for ticker in companies:\n",
        "        print(f\"Loading data for {ticker}...\")  # Debug statement\n",
        "        data = load_company_data(ticker)\n",
        "\n",
        "        if data.empty:\n",
        "            print(f\"Skipping {ticker} due to empty or missing data.\")\n",
        "            continue\n",
        "\n",
        "        # Use relevant features for the model (e.g., Close price, Volume)\n",
        "        features = data[['Close', 'Volume']].fillna(0).values.astype('float32')\n",
        "        labels = data[['Close']].fillna(0).values.astype('float32')\n",
        "\n",
        "        # Create dataset and check batches\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "        dataset = dataset.batch(32)  # Batch the dataset\n",
        "        client_data.append(dataset)\n",
        "\n",
        "        # Debug: Check batched dataset\n",
        "        num_batches = len(list(dataset))\n",
        "        print(f\"Batched dataset for {ticker} contains {num_batches} batches.\")\n",
        "\n",
        "    return client_data\n",
        "\n",
        "# Start the client data loading process\n",
        "client_data = create_client_data()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb6r1_Cw7w4c",
        "outputId": "e28bee6e-e4f8-4751-f442-ab9be12c23da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data for DE...\n",
            "Data loaded for DE: 1257 rows.\n",
            "Batched dataset for DE contains 40 batches.\n",
            "Loading data for ADM...\n",
            "Data loaded for ADM: 1257 rows.\n",
            "Batched dataset for ADM contains 40 batches.\n",
            "Loading data for BG...\n",
            "Data loaded for BG: 1257 rows.\n",
            "Batched dataset for BG contains 40 batches.\n",
            "Loading data for MOS...\n",
            "Data loaded for MOS: 1257 rows.\n",
            "Batched dataset for MOS contains 40 batches.\n",
            "Loading data for CTVA...\n",
            "Data loaded for CTVA: 1257 rows.\n",
            "Batched dataset for CTVA contains 40 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if len(list(dataset)) == 0:\n",
        "    print(f\"No batches for {ticker}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "yK8dAx097FOu",
        "outputId": "913b2fb8-d219-465f-fd54-33858f42ba22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1466f8ecd8c6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No batches for {ticker}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "lNid8o777Hfm",
        "outputId": "7c8c08f6-0e2b-414b-e13b-926c957e3040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Mounting drive is unsupported in this environment. Use PyDrive instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[0;34m\"\"\"Internal helper to mount Google Drive.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/var/colab/hostname'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;34m'Mounting drive is unsupported in this environment. Use PyDrive'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;34m' instead. See examples at'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Mounting drive is unsupported in this environment. Use PyDrive instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to the saved stock data in Colab\n",
        "base_dir = '/content/FinancialData'\n",
        "\n",
        "# Load each company's stock data from the saved CSV files\n",
        "def load_company_data(ticker):\n",
        "    file_path = os.path.join(base_dir, f\"{ticker}_stock_data.csv\")\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Define a function to load stock data for all companies and convert it to tf.data.Dataset\n",
        "def create_client_data():\n",
        "    companies = ['DE', 'ADM', 'BG', 'MOS', 'CTVA']  # List of ticker symbols\n",
        "    client_data = []\n",
        "\n",
        "    for ticker in companies:\n",
        "        data = load_company_data(ticker)\n",
        "        # Debug: Check if data is loaded correctly\n",
        "        #print(f\"Data for {ticker} loaded. Sample:\\n\", data.head())\n",
        "\n",
        "        # Use relevant features for the model (e.g., Close price, Volume)\n",
        "        features = data[['Close', 'Volume']].fillna(0).values.astype('float32')  # Changed to float32\n",
        "        labels = data[['Close']].fillna(0).values.astype('float32')  # Predicting Close price, changed to float32\n",
        "\n",
        "        # Debug: Check features and labels\n",
        "        #print(f\"Features shape for {ticker}: {features.shape}\")\n",
        "        #print(f\"Labels shape for {ticker}: {labels.shape}\")\n",
        "\n",
        "        # Create dataset and check batches\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "        dataset = dataset.batch(32)  # Batch the dataset\n",
        "        client_data.append(dataset)\n",
        "\n",
        "        # Debug: Check batched dataset\n",
        "        #print(f\"Batched dataset for {ticker}:\")\n",
        "        for batch in dataset:\n",
        "            #print(batch)\n",
        "            break  # Print only the first batch for debugging\n",
        "\n",
        "    return client_data\n",
        "\n",
        "# Create a simple model for stock price prediction (e.g., linear regression)\n",
        "def create_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu', input_shape=(2,)),  # 2 features: Close price, Volume\n",
        "        tf.keras.layers.Dense(1)  # Output layer for predicting next price\n",
        "    ])\n",
        "\n",
        "    # Debug: Check model summary\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "# Define a TFF model\n",
        "def model_fn():\n",
        "    try:\n",
        "        # Create the Keras model using the function defined earlier\n",
        "        keras_model = create_model()\n",
        "\n",
        "        # Ensure input_spec matches the dataset structure (features, labels)\n",
        "        input_spec = (\n",
        "            tf.TensorSpec(shape=[None, 2], dtype=tf.float32),  # Features (2: Close price, Volume)\n",
        "            tf.TensorSpec(shape=[None, 1], dtype=tf.float32)   # Labels (1: Close price prediction)\n",
        "        )\n",
        "\n",
        "        # Create a TFF model from the Keras model\n",
        "        tff_model = tff.learning.models.from_keras_model(\n",
        "            keras_model=keras_model,\n",
        "            input_spec=input_spec,\n",
        "            loss=tf.keras.losses.MeanSquaredError(),  # Loss function (regression task)\n",
        "            metrics=[tf.keras.metrics.MeanSquaredError()]  # Metrics to track\n",
        "        )\n",
        "\n",
        "        print(\"TFF model created successfully.\")\n",
        "\n",
        "        return tff_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while creating TFF model: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Federated learning process setup\n",
        "def federated_training():\n",
        "    client_data = create_client_data()\n",
        "\n",
        "    # Use TFF's internal sgdm optimizer instead of Keras's optimizer\n",
        "    client_optimizer_fn = tff.learning.optimizers.build_sgdm(learning_rate=0.01)\n",
        "    server_optimizer_fn = tff.learning.optimizers.build_sgdm(learning_rate=1.0)\n",
        "\n",
        "    # Build federated averaging process using TFF's internal optimizers\n",
        "    iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "        model_fn=model_fn,\n",
        "        client_optimizer_fn=client_optimizer_fn,  # Pass the optimizer directly\n",
        "        server_optimizer_fn=server_optimizer_fn   # Pass the optimizer directly\n",
        "    )\n",
        "\n",
        "    # Initialize the model state\n",
        "    state = iterative_process.initialize()\n",
        "\n",
        "    # Simulate training across clients (5 companies)\n",
        "    for round_num in range(1, 11):\n",
        "        state, metrics = iterative_process.next(state, client_data)\n",
        "        print(f'Round {round_num}, Metrics: {metrics}')\n",
        "\n",
        "# Start federated training\n",
        "federated_training()\n"
      ],
      "metadata": {
        "id": "p8Jj7ZlHipik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ae0c20c-173f-4ada-f67e-fec9ebcc7129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                30        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41 (164.00 Byte)\n",
            "Trainable params: 41 (164.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "TFF model created successfully.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                30        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41 (164.00 Byte)\n",
            "Trainable params: 41 (164.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "TFF model created successfully.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 10)                30        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41 (164.00 Byte)\n",
            "Trainable params: 41 (164.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "TFF model created successfully.\n",
            "Round 1, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 2, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 3, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 4, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 5, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 6, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 7, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 8, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 9, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n",
            "Round 10, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('mean_squared_error', 0.0), ('loss', 0.0), ('num_examples', 0), ('num_batches', 0)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 1)]))])\n"
          ]
        }
      ]
    }
  ]
}